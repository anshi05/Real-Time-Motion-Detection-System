{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9732498,"sourceType":"datasetVersion","datasetId":5956090},{"sourceId":9735641,"sourceType":"datasetVersion","datasetId":5958461}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-13T17:16:47.287025Z","iopub.execute_input":"2024-11-13T17:16:47.287526Z","iopub.status.idle":"2024-11-13T17:16:47.304127Z","shell.execute_reply.started":"2024-11-13T17:16:47.287479Z","shell.execute_reply":"2024-11-13T17:16:47.302522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2 as cv\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom scipy import signal\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2024-11-13T17:16:47.310819Z","iopub.execute_input":"2024-11-13T17:16:47.311413Z","iopub.status.idle":"2024-11-13T17:16:47.321044Z","shell.execute_reply.started":"2024-11-13T17:16:47.311362Z","shell.execute_reply":"2024-11-13T17:16:47.319425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"video_file = '/kaggle/input/vids-for-motion-detection/Normal_Videos209_x264.mp4'","metadata":{"execution":{"iopub.status.busy":"2024-11-13T17:16:47.323150Z","iopub.execute_input":"2024-11-13T17:16:47.323548Z","iopub.status.idle":"2024-11-13T17:16:47.337059Z","shell.execute_reply.started":"2024-11-13T17:16:47.323505Z","shell.execute_reply":"2024-11-13T17:16:47.335815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ffmpeg -hide_banner -i /kaggle/input/vids-for-motion-detection/Normal_Videos209_x264.mp4","metadata":{"execution":{"iopub.status.busy":"2024-11-13T17:16:47.389858Z","iopub.execute_input":"2024-11-13T17:16:47.390714Z","iopub.status.idle":"2024-11-13T17:16:48.639728Z","shell.execute_reply.started":"2024-11-13T17:16:47.390656Z","shell.execute_reply":"2024-11-13T17:16:48.638149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"capture = cv.VideoCapture(video_file)\nprint(f'FPS: {capture.get(cv.CAP_PROP_FPS)}')\nprint(f'Start offset: {capture.get(cv.CAP_PROP_POS_FRAMES)}')\nprint(f'Total frames: {int(capture.get(cv.CAP_PROP_FRAME_COUNT))}')\nprint(f'Total seconds: {int(capture.get(cv.CAP_PROP_FRAME_COUNT) / (capture.get(cv.CAP_PROP_FPS)))}')","metadata":{"execution":{"iopub.status.busy":"2024-11-13T17:16:48.642930Z","iopub.execute_input":"2024-11-13T17:16:48.643419Z","iopub.status.idle":"2024-11-13T17:16:48.665320Z","shell.execute_reply.started":"2024-11-13T17:16:48.643367Z","shell.execute_reply":"2024-11-13T17:16:48.664046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create background subtractor\nbackSub = cv.createBackgroundSubtractorKNN()\n\nfgMask = None\nmasks = []\n\nwith tqdm(total=capture.get(cv.CAP_PROP_FRAME_COUNT)) as pbar:\n    while True:\n        ret, frame = capture.read()\n    \n        if frame is None:\n            break\n\n        pbar.update(1)\n    \n        fgMask = backSub.apply(frame)\n        \n        masks.append(fgMask[fgMask > 0].shape[0])","metadata":{"execution":{"iopub.status.busy":"2024-11-13T17:16:48.667083Z","iopub.execute_input":"2024-11-13T17:16:48.667542Z","iopub.status.idle":"2024-11-13T17:16:50.895767Z","shell.execute_reply.started":"2024-11-13T17:16:48.667495Z","shell.execute_reply":"2024-11-13T17:16:50.894600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({'signal': masks})","metadata":{"execution":{"iopub.status.busy":"2024-11-13T17:16:50.898662Z","iopub.execute_input":"2024-11-13T17:16:50.899073Z","iopub.status.idle":"2024-11-13T17:16:50.906392Z","shell.execute_reply.started":"2024-11-13T17:16:50.899033Z","shell.execute_reply":"2024-11-13T17:16:50.905064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = df.plot(figsize=(14, 4))\nax.grid(axis='both', which='both')\nplt.minorticks_on()\nplt.ylim([0, 100000])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-13T17:16:50.907750Z","iopub.execute_input":"2024-11-13T17:16:50.908137Z","iopub.status.idle":"2024-11-13T17:16:51.413172Z","shell.execute_reply.started":"2024-11-13T17:16:50.908088Z","shell.execute_reply":"2024-11-13T17:16:51.412003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"origin = lambda x: x\nbilateral = lambda x, sigma=1<<5: cv.bilateralFilter(x, sigma, sigma * 2, sigma / 2)\ngaussian = lambda x: cv.GaussianBlur(x, (5, 5), 0)\nblur = lambda x: cv.blur(frame, (5, 5))\nmedian = lambda x: cv.medianBlur(frame, 5)\n\nfilters = {\n    'original': {'filter': origin, 'subtractor': cv.createBackgroundSubtractorKNN(), 'signal': []},\n    'bilateral': {'filter': bilateral, 'subtractor': cv.createBackgroundSubtractorKNN(), 'signal': []},\n    'blur': {'filter': blur, 'subtractor': cv.createBackgroundSubtractorKNN(), 'signal': []},\n    'gaussian': {'filter': gaussian, 'subtractor': cv.createBackgroundSubtractorKNN(), 'signal': []},\n    'median': {'filter': median, 'subtractor': cv.createBackgroundSubtractorKNN(), 'signal': []},\n}\n\ncapture = cv.VideoCapture(video_file)\nwith tqdm(total=capture.get(cv.CAP_PROP_FRAME_COUNT)) as pbar:\n    while True:\n        # Read frame\n        ret, frame = capture.read()\n    \n        # EOF\n        if frame is None:\n            break\n        pbar.update(1)\n    \n        for filter_name in filters:\n            filtered_frame = filters[filter_name]['filter'](frame)\n            mask = filters[filter_name]['subtractor'].apply(filtered_frame)\n            filters[filter_name]['signal'].append(mask[mask > 0].shape[0])","metadata":{"execution":{"iopub.status.busy":"2024-11-13T17:16:51.414671Z","iopub.execute_input":"2024-11-13T17:16:51.415122Z","iopub.status.idle":"2024-11-13T17:19:01.131045Z","shell.execute_reply.started":"2024-11-13T17:16:51.415076Z","shell.execute_reply":"2024-11-13T17:19:01.129717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({\n    'orig': filters['original']['signal'],\n    'bilateral': filters['bilateral']['signal'],\n    'blur': filters['blur']['signal'],\n    'gaussian': filters['gaussian']['signal'],\n    'median': filters['median']['signal'],\n})\n\nax = df.plot(figsize=(14, 4))\nax.grid(axis='both', which='both')\nplt.minorticks_on()\nplt.ylim([0, 100000])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-13T17:19:01.133156Z","iopub.execute_input":"2024-11-13T17:19:01.133666Z","iopub.status.idle":"2024-11-13T17:19:01.617260Z","shell.execute_reply.started":"2024-11-13T17:19:01.133589Z","shell.execute_reply":"2024-11-13T17:19:01.615694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Kalman\nkalman_arr = []\nkalman = cv.KalmanFilter(2, 1)\nkalman.measurementMatrix = np.array([[1, 0]], np.float32)\nkalman.transitionMatrix = np.array([[1, 0], [0, 1]], np.float32)\nkalman.processNoiseCov = np.array([[1, 0], [0, 1]], np.float32) * 0.03\n\n# Add kalman data storage\nfilters.update({\n    'kalman': {'filter': None, 'subtractor': None, 'signal': []},\n})\n# Reset blur filter data storage\nfilters['blur']['signal'] = []\n\ncapture = cv.VideoCapture(video_file)\n\nwith tqdm(total=capture.get(cv.CAP_PROP_FRAME_COUNT)) as pbar:\n    while True:\n        # Read frame\n        ret, frame = capture.read()\n    \n        # EOF\n        if frame is None:\n            break\n        pbar.update(1)\n    \n        filtered_frame = filters['blur']['filter'](frame)\n        mask = filters[filter_name]['subtractor'].apply(filtered_frame)\n        filters['blur']['signal'].append(mask[mask > 0].shape[0])\n\n        measurement = np.array([np.float32(mask[mask > 0].shape[0])])\n\n        kalman.correct(measurement)\n        prediction = kalman.predict()\n        filters['kalman']['signal'].append(prediction[0][0])","metadata":{"execution":{"iopub.status.busy":"2024-11-13T17:19:01.619140Z","iopub.execute_input":"2024-11-13T17:19:01.619646Z","iopub.status.idle":"2024-11-13T17:19:04.311907Z","shell.execute_reply.started":"2024-11-13T17:19:01.619572Z","shell.execute_reply":"2024-11-13T17:19:04.310316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({\n    'orig': filters['original']['signal'],\n    'blur': filters['blur']['signal'],\n    'kalman': filters['kalman']['signal'],\n})\n\nax = df.plot(figsize=(14, 4))\nax.grid(axis='both', which='both')\nplt.minorticks_on()\nplt.ylim([0, 100000])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-13T17:19:04.313747Z","iopub.execute_input":"2024-11-13T17:19:04.314185Z","iopub.status.idle":"2024-11-13T17:19:04.836094Z","shell.execute_reply.started":"2024-11-13T17:19:04.314138Z","shell.execute_reply":"2024-11-13T17:19:04.834710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2 as cv\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\ndef process_video(video_file):\n    capture = cv.VideoCapture(video_file)\n    print(f'Processing video: {video_file}')\n    print(f'FPS: {capture.get(cv.CAP_PROP_FPS)}')\n    print(f'Total frames: {int(capture.get(cv.CAP_PROP_FRAME_COUNT))}')\n    \n    backSub = cv.createBackgroundSubtractorKNN()\n    \n    masks = []\n    \n    # First pass: Get motion detection signals\n    with tqdm(total=capture.get(cv.CAP_PROP_FRAME_COUNT)) as pbar:\n        while True:\n            ret, frame = capture.read()\n            if not ret:  # Check if frame is valid\n                break\n            pbar.update(1)\n            fgMask = backSub.apply(frame)\n            masks.append(fgMask[fgMask > 0].shape[0])\n    \n    df = pd.DataFrame({'signal': masks})\n    ax = df.plot(figsize=(14, 4))\n    ax.grid(axis='both', which='both')\n    plt.minorticks_on()\n    plt.ylim([0, 400000])\n    plt.title(f'Motion Detection Signal for {video_file}')\n    plt.show()\n\n    # Kalman Filter setup\n    kalman_arr = []\n    kalman = cv.KalmanFilter(2, 1)\n    kalman.measurementMatrix = np.array([[1, 0]], np.float32)\n    kalman.transitionMatrix = np.array([[1, 0], [0, 1]], np.float32)\n    kalman.processNoiseCov = np.array([[1, 0], [0, 1]], np.float32) * 0.03\n\n    # Filtering section with only blur and Kalman\n    filters = {\n        'blur': {'filter': lambda x: cv.blur(x, (5, 5)), 'subtractor': backSub, 'signal': []},\n        'kalman': {'filter': None, 'subtractor': None, 'signal': []},\n    }\n\n    # Reset the capture to start again\n    capture.set(cv.CAP_PROP_POS_FRAMES, 0)\n\n    # Second pass: Apply filters and get signals\n    with tqdm(total=capture.get(cv.CAP_PROP_FRAME_COUNT)) as pbar:\n        while True:\n            ret, frame = capture.read()\n            if not ret:  # Check if frame is valid\n                break\n            pbar.update(1)\n\n            # Apply blur filter\n            filtered_frame = filters['blur']['filter'](frame)\n            mask = filters['blur']['subtractor'].apply(filtered_frame)\n            filters['blur']['signal'].append(mask[mask > 0].shape[0])\n\n            # Kalman filter prediction\n#             measurement = np.array([np.float32(mask[mask > 0].shape[0])])\n#             kalman.correct(measurement)\n#             prediction = kalman.predict()\n#             filters['kalman']['signal'].append(prediction[0][0])\n\n    df = pd.DataFrame({\n        'blur': filters['blur']['signal'],\n#         'kalman': filters['kalman']['signal'],\n    })\n\n    ax = df.plot(figsize=(14, 4))\n    ax.grid(axis='both', which='both')\n    plt.minorticks_on()\n    plt.ylim([0, 400000])\n    plt.title(f'Blur Signals for {video_file}')\n    plt.show()\n\n# List of video files\nvideo_files = [\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos020_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos071_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos093_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos099_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos114_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos149_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos162_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos178_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos209_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos222_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos236_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos243_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos244_x264.mp4',\n]\n\n# Loop through all video files\nfor video_file in video_files:\n    process_video(video_file)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T17:19:04.841067Z","iopub.execute_input":"2024-11-13T17:19:04.841642Z","iopub.status.idle":"2024-11-13T17:21:17.543330Z","shell.execute_reply.started":"2024-11-13T17:19:04.841561Z","shell.execute_reply":"2024-11-13T17:21:17.541937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2 as cv\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\ndef calculate_snr(signal):\n    \"\"\"Calculate Signal-to-Noise Ratio (SNR).\"\"\"\n    signal_mean = np.mean(signal)\n    noise_std = np.std(signal)\n    if noise_std == 0:  # Avoid division by zero\n        return float('inf')  # High SNR if there's no noise\n    return 10 * np.log10(signal_mean / noise_std)\n\ndef process_video(video_file):\n    capture = cv.VideoCapture(video_file)\n    print(f'Processing video: {video_file}')\n    print(f'FPS: {capture.get(cv.CAP_PROP_FPS)}')\n    print(f'Total frames: {int(capture.get(cv.CAP_PROP_FRAME_COUNT))}')\n    \n    backSub = cv.createBackgroundSubtractorKNN()\n    \n    masks = []\n    \n    # First pass: Get motion detection signals\n    with tqdm(total=capture.get(cv.CAP_PROP_FRAME_COUNT)) as pbar:\n        while True:\n            ret, frame = capture.read()\n            if not ret:  # Check if frame is valid\n                break\n            pbar.update(1)\n            fgMask = backSub.apply(frame)\n            masks.append(fgMask[fgMask > 0].shape[0])\n    \n    # Calculate SNR for the raw signal\n    raw_snr = calculate_snr(masks)\n    print(f'Raw SNR for {video_file}: {raw_snr:.2f} dB')\n    \n    df = pd.DataFrame({'signal': masks})\n    ax = df.plot(figsize=(14, 4))\n    ax.grid(axis='both', which='both')\n    plt.minorticks_on()\n    plt.ylim([0, 400000])\n    plt.title(f'Motion Detection Signal for {video_file}')\n    plt.show()\n\n    # Filtering section\n    filters = {\n        'blur': {'filter': lambda x: cv.blur(x, (5, 5)), 'subtractor': backSub, 'signal': []},\n    }\n\n    # Reset the capture to start again\n    capture.set(cv.CAP_PROP_POS_FRAMES, 0)\n\n    # Second pass: Apply filters and get signals\n    with tqdm(total=capture.get(cv.CAP_PROP_FRAME_COUNT)) as pbar:\n        while True:\n            ret, frame = capture.read()\n            if not ret:  # Check if frame is valid\n                break\n            pbar.update(1)\n\n            # Apply blur filter\n            filtered_frame = filters['blur']['filter'](frame)\n            mask = filters['blur']['subtractor'].apply(filtered_frame)\n            filters['blur']['signal'].append(mask[mask > 0].shape[0])\n\n    # Calculate SNR for the blurred signal\n    blurred_signal = filters['blur']['signal']\n    blurred_snr = calculate_snr(blurred_signal)\n    print(f'Blurred SNR for {video_file}: {blurred_snr:.2f} dB')\n\n    # Create a table to display SNR values\n    snr_df = pd.DataFrame({\n        'Metric': ['Raw SNR', 'Blurred SNR'],\n        'Value (dB)': [raw_snr, blurred_snr]\n    })\n    \n    print(\"\\nSNR Comparison Table:\")\n    print(snr_df)\n\n    # Plotting the blurred signal\n    df = pd.DataFrame({\n        'blur': filters['blur']['signal'],\n    })\n\n    ax = df.plot(figsize=(14, 4))\n    ax.grid(axis='both', which='both')\n    plt.minorticks_on()\n    plt.ylim([0, 400000])\n    plt.title(f'Blur Signals for {video_file}')\n    plt.show()\n\n# List of video files\nvideo_files = [\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos020_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos071_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos093_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos099_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos114_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos149_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos162_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos178_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos209_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos222_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos236_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos243_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos244_x264.mp4',    \n]\n\n# Loop through all video files\nfor video_file in video_files:\n    process_video(video_file)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T17:21:17.545546Z","iopub.execute_input":"2024-11-13T17:21:17.546075Z","iopub.status.idle":"2024-11-13T17:23:30.616823Z","shell.execute_reply.started":"2024-11-13T17:21:17.546015Z","shell.execute_reply":"2024-11-13T17:23:30.615642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2 as cv\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\ndef process_video(video_file):\n    capture = cv.VideoCapture(video_file)\n    print(f'Processing video: {video_file}')\n    print(f'FPS: {capture.get(cv.CAP_PROP_FPS)}')\n    print(f'Total frames: {int(capture.get(cv.CAP_PROP_FRAME_COUNT))}')\n    \n    backSub = cv.createBackgroundSubtractorKNN()\n    \n    masks = []\n    \n    # First pass: Get motion detection signals\n    with tqdm(total=capture.get(cv.CAP_PROP_FRAME_COUNT)) as pbar:\n        while True:\n            ret, frame = capture.read()\n            if not ret:  # Check if frame is valid\n                break\n            pbar.update(1)\n            fgMask = backSub.apply(frame)\n            masks.append(fgMask[fgMask > 0].shape[0])\n    \n    df = pd.DataFrame({'signal': masks})\n    ax = df.plot(figsize=(14, 4))\n    ax.grid(axis='both', which='both')\n    plt.minorticks_on()\n    plt.ylim([0, 400000])\n    plt.title(f'Motion Detection Signal for {video_file}')\n    plt.show()\n\n    # Kalman Filter setup\n    kalman_arr = []\n    kalman = cv.KalmanFilter(2, 1)\n    kalman.measurementMatrix = np.array([[1, 0]], np.float32)\n    kalman.transitionMatrix = np.array([[1, 0], [0, 1]], np.float32)\n    kalman.processNoiseCov = np.array([[1, 0], [0, 1]], np.float32) * 0.03\n\n    # Filtering section with only blur and Kalman\n    filters = {\n        'blur': {'filter': lambda x: cv.blur(x, (5, 5)), 'subtractor': backSub, 'signal': []},\n        'kalman': {'filter': None, 'subtractor': None, 'signal': []},\n    }\n\n    # Reset the capture to start again\n    capture.set(cv.CAP_PROP_POS_FRAMES, 0)\n\n    # Second pass: Apply filters and get signals\n    with tqdm(total=capture.get(cv.CAP_PROP_FRAME_COUNT)) as pbar:\n        while True:\n            ret, frame = capture.read()\n            if not ret:  # Check if frame is valid\n                break\n            pbar.update(1)\n\n            # Apply blur filter\n            filtered_frame = filters['blur']['filter'](frame)\n            mask = filters['blur']['subtractor'].apply(filtered_frame)\n            filters['blur']['signal'].append(mask[mask > 0].shape[0])\n\n            # Kalman filter prediction\n            measurement = np.array([np.float32(mask[mask > 0].shape[0])])\n            kalman.correct(measurement)\n            prediction = kalman.predict()\n            filters['kalman']['signal'].append(prediction[0][0])\n\n    df = pd.DataFrame({\n        'blur': filters['blur']['signal'],\n        'kalman': filters['kalman']['signal'],\n    })\n\n    ax = df.plot(figsize=(14, 4))\n    ax.grid(axis='both', which='both')\n    plt.minorticks_on()\n    plt.ylim([0, 400000])\n    plt.title(f'Blur Signals for {video_file}')\n    plt.show()\n\n# List of video files\nvideo_files = [\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos020_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos071_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos093_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos099_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos114_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos149_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos162_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos178_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos209_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos222_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos236_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos243_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos244_x264.mp4',\n]\n\n# Loop through all video files\nfor video_file in video_files:\n    process_video(video_file)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T17:23:30.618787Z","iopub.execute_input":"2024-11-13T17:23:30.619163Z","iopub.status.idle":"2024-11-13T17:25:55.536393Z","shell.execute_reply.started":"2024-11-13T17:23:30.619122Z","shell.execute_reply":"2024-11-13T17:25:55.534896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2 as cv\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\ndef process_video(video_file, threshold=1000):\n    capture = cv.VideoCapture(video_file)\n    print(f'Processing video: {video_file}')\n    print(f'FPS: {capture.get(cv.CAP_PROP_FPS)}')\n    print(f'Total frames: {int(capture.get(cv.CAP_PROP_FRAME_COUNT))}')\n    \n    backSub = cv.createBackgroundSubtractorKNN()\n    \n    masks = []\n    \n    # First pass: Get motion detection signals\n    with tqdm(total=capture.get(cv.CAP_PROP_FRAME_COUNT)) as pbar:\n        while True:\n            ret, frame = capture.read()\n            if not ret:  # Check if frame is valid\n                break\n            pbar.update(1)\n            fgMask = backSub.apply(frame)\n            masks.append(fgMask[fgMask > 0].shape[0])\n    \n    # Initialize Kalman Filter\n    kalman = cv.KalmanFilter(2, 1)\n    kalman.measurementMatrix = np.array([[1, 0]], np.float32)\n    kalman.transitionMatrix = np.array([[1, 0], [0, 1]], np.float32)\n    kalman.processNoiseCov = np.array([[1, 0], [0, 1]], np.float32) * 0.03\n\n    # Filtering section with blur and Kalman\n    filters = {\n        'blur': {'filter': lambda x: cv.blur(x, (5, 5)), 'subtractor': backSub, 'signal': []},\n        'kalman': {'signal': []},\n    }\n\n    # Reset the capture to start again\n    capture.set(cv.CAP_PROP_POS_FRAMES, 0)\n\n    # Second pass: Apply filters and get signals\n    with tqdm(total=capture.get(cv.CAP_PROP_FRAME_COUNT)) as pbar:\n        while True:\n            ret, frame = capture.read()\n            if not ret:  # Check if frame is valid\n                break\n            pbar.update(1)\n\n            # Apply blur filter\n            filtered_frame = filters['blur']['filter'](frame)\n            mask = filters['blur']['subtractor'].apply(filtered_frame)\n            filters['blur']['signal'].append(mask[mask > 0].shape[0])\n\n            # Kalman filter prediction\n            measurement = np.array([np.float32(mask[mask > 0].shape[0])])\n            kalman.correct(measurement)\n            prediction = kalman.predict()\n            filters['kalman']['signal'].append(prediction[0][0])\n\n    df = pd.DataFrame({\n        'blur': filters['blur']['signal'],\n        'kalman': filters['kalman']['signal'],\n    })\n\n    # Apply thresholding\n    df['motion_detected_blur'] = df['blur'] > threshold\n    df['motion_detected_kalman'] = df['kalman'] > threshold\n\n    # Calculate True Positives, False Positives, False Negatives\n    # For example, let's assume you have a true motion label array `true_labels`\n    # true_labels = ... # This should be a boolean array indicating true motion (1) or not (0)\n    # TP = np.sum(df['motion_detected_blur'] & true_labels)\n    # FP = np.sum(df['motion_detected_blur'] & ~true_labels)\n    # FN = np.sum(~df['motion_detected_blur'] & true_labels)\n\n    # Plotting the signals\n    ax = df[['blur', 'kalman']].plot(figsize=(14, 4), alpha=0.7)\n    ax.fill_between(df.index, 0, df['blur'], where=df['motion_detected_blur'], color='yellow', alpha=0.5, label='Detected Motion (Blur)')\n    ax.fill_between(df.index, 0, df['kalman'], where=df['motion_detected_kalman'], color='green', alpha=0.5, label='Detected Motion (Kalman)')\n    ax.axhline(y=threshold, color='red', linestyle='--', label='Threshold')\n    \n    plt.title(f'Blur and Kalman Filter Signals with Motion Detection for {video_file}')\n    plt.xlabel('Frame Index')\n    plt.ylabel('Signal Amplitude')\n    plt.legend()\n    plt.grid()\n    plt.show()\n\n# List of video files\nvideo_files = [\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos020_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos071_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos093_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos099_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos114_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos149_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos162_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos178_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos209_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos222_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos236_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos243_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos244_x264.mp4',    \n]\n\n# Loop through all video files\nfor video_file in video_files:\n    process_video(video_file)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T17:25:55.538518Z","iopub.execute_input":"2024-11-13T17:25:55.539130Z","iopub.status.idle":"2024-11-13T17:28:11.657201Z","shell.execute_reply.started":"2024-11-13T17:25:55.539064Z","shell.execute_reply":"2024-11-13T17:28:11.655505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2 as cv\nimport pandas as pd\nimport numpy as np\n\ndef label_video_frames(video_file, threshold=1000):\n    capture = cv.VideoCapture(video_file)\n    frame_labels = []\n\n    # Background subtractor and Kalman filter initialization\n    backSub = cv.createBackgroundSubtractorKNN()\n    kalman = cv.KalmanFilter(2, 1)\n    kalman.measurementMatrix = np.array([[1, 0]], np.float32)\n    kalman.transitionMatrix = np.array([[1, 0], [0, 1]], np.float32)\n    kalman.processNoiseCov = np.array([[1, 0], [0, 1]], np.float32) * 0.03\n\n    while True:\n        ret, frame = capture.read()\n        if not ret:\n            break\n\n        # Apply blur filter\n        blurred_frame = cv.blur(frame, (5, 5))\n        \n        # Apply background subtraction\n        fgMask = backSub.apply(blurred_frame)\n        motion_pixels = np.sum(fgMask > 0)\n\n        # Kalman filter prediction and correction\n        measurement = np.array([[np.float32(motion_pixels)]])\n        kalman.correct(measurement)\n        prediction = kalman.predict()\n\n        # Label based on motion pixels after Kalman filtering\n        label = 1 if prediction[0][0] > threshold else 0\n        frame_labels.append((capture.get(cv.CAP_PROP_POS_FRAMES), label))\n\n    capture.release()\n    return frame_labels\n\n# Example usage\nvideo_files = [\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos020_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos071_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos093_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos099_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos114_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos149_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos162_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos178_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos209_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos222_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos236_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos243_x264.mp4',\n    '/kaggle/input/vids-for-motion-detection/Normal_Videos244_x264.mp4',\n]\n\nall_labels = {}\nfor video_file in video_files:\n    labels = label_video_frames(video_file)\n    all_labels[video_file] = labels\n\n# Convert to DataFrame for further use\nlabel_df = pd.DataFrame([(vid, frame, label) for vid, frames in all_labels.items() for frame, label in frames], \n                        columns=['video_id', 'frame_index', 'label'])\n\n# Save to CSV\nlabel_df.to_csv('motion_detected_labels.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T17:28:11.659098Z","iopub.execute_input":"2024-11-13T17:28:11.659532Z","iopub.status.idle":"2024-11-13T17:29:14.899488Z","shell.execute_reply.started":"2024-11-13T17:28:11.659485Z","shell.execute_reply":"2024-11-13T17:29:14.898292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load true labels and detected labels\ntrue_labels_df = pd.read_csv('/kaggle/input/ground-truth-labels/ground_truth_labels.csv')  # Replace with your true labels CSV\ndetected_labels_df = pd.read_csv('/kaggle/working/motion_detected_labels.csv')  # The one generated with your labeling code\n\n# Merge DataFrames on 'video_id' and 'frame_index'\nmerged_df = pd.merge(true_labels_df, detected_labels_df, on=['video_id', 'frame_index'], suffixes=('_true', '_detected'))\n\n# Initialize a dictionary to store confusion matrices and metrics for each video\nresults = {}\n\n# Initialize lists to store true and detected labels for overall metrics\nall_y_true = []\nall_y_pred = []\n\n# Iterate through each unique video\nfor video_id in merged_df['video_id'].unique():\n    video_data = merged_df[merged_df['video_id'] == video_id]\n    \n    # Extract true and detected labels\n    y_true = video_data['label_true'].values\n    y_pred = video_data['label_detected'].values\n    \n    # Store all true and predicted labels for overall metrics\n    all_y_true.extend(y_true)\n    all_y_pred.extend(y_pred)\n    \n    # Calculate confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Calculate performance metrics\n    report = classification_report(y_true, y_pred, output_dict=True)\n    \n    # Store results\n    results[video_id] = {\n        'confusion_matrix': cm,\n        'accuracy': report['accuracy'],\n        'precision': report['1']['precision'],  # Assuming 1 is the positive class\n        'recall': report['1']['recall'],\n        'f1-score': report['1']['f1-score']\n    }\n\n# Convert results to DataFrame for better readability\nresults_df = pd.DataFrame.from_dict(results, orient='index')\n\n# Calculate overall performance metrics\noverall_accuracy = accuracy_score(all_y_true, all_y_pred)\noverall_precision = precision_score(all_y_true, all_y_pred, average='binary', pos_label=1)\noverall_recall = recall_score(all_y_true, all_y_pred, average='binary', pos_label=1)\noverall_f1_score = f1_score(all_y_true, all_y_pred, average='binary', pos_label=1)\n\n# Print overall performance metrics\nprint(\"Overall Performance Metrics:\")\nprint(f\"Accuracy: {overall_accuracy:.2f}\")\nprint(f\"Precision: {overall_precision:.2f}\")\nprint(f\"Recall: {overall_recall:.2f}\")\nprint(f\"F1 Score: {overall_f1_score:.2f}\")\n\n# Plotting Confusion Matrices and Metrics\nfor video_id, metrics in results.items():\n    # Plot confusion matrix\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues', cbar=False)\n    plt.title(f'Confusion Matrix for {video_id}')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.xticks(ticks=[0.5, 1.5], labels=['No Motion', 'Motion'])\n    plt.yticks(ticks=[0.5, 1.5], labels=['No Motion', 'Motion'])\n    plt.show()\n\n    # Plot performance metrics\n    plt.figure(figsize=(8, 5))\n    metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1-score']\n    scores = [metrics[m] for m in metrics_to_plot]\n    \n    bars = plt.bar(metrics_to_plot, scores, color=['blue', 'orange', 'green', 'red'])\n    plt.ylim(0, 1)\n    plt.title(f'Performance Metrics for {video_id}')\n    plt.ylabel('Score')\n    \n    # Adding score annotations inside the bars\n    for bar in bars:\n        yval = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2, yval / 2, f'{yval:.2f}', ha='center', va='center', color='white')\n    \n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T17:29:14.901031Z","iopub.execute_input":"2024-11-13T17:29:14.901401Z","iopub.status.idle":"2024-11-13T17:29:21.721896Z","shell.execute_reply.started":"2024-11-13T17:29:14.901360Z","shell.execute_reply":"2024-11-13T17:29:21.720602Z"},"trusted":true},"execution_count":null,"outputs":[]}]}